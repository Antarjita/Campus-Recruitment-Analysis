---
title: "Exploratory Data Analysis"
output:
  html_document:
    df_print: paged
---
---
Importing the libraries
```{r}
library(randomForest)
library(caTools)
library(caret)
library(dplyr)
```
Reading the dataset
```{r}
set.seed(3456)
data<-read.csv("Placement_Data_Full_Class.csv")
head(data)
```
Data Cleaning and Data Pre-processing
1.Dropping the first column 
```{r}
data<-select(data,-1)
head(data)
```

2.Removing Null Values
```{r}
data$salary[is.na(data$salary)]<-0
head(data)
```
3.Normalization
```{r}
normalize<-function(attribute){ return ( (attribute - min(attribute))/(max(attribute)-min(attribute))) }
data$ssc_p<-normalize(data$ssc_p)
data$hsc_p<-normalize(data$hsc_p)
data$degree_p<-normalize(data$degree_p)
data$etest_p<-normalize(data$etest_p)
data$mba_p<-normalize(data$mba_p)
data$salary<-normalize(data$salary)
head(data)
```
Data preview
```{r}
str(data)
```
Handling categorical variables
```{r}
data$status=as.numeric(as.factor(data$status))-1
data$gender=as.numeric(as.factor(data$gender))-1
data$specialisation=as.numeric(as.factor(data$specialisation))-1
data$ssc_b=as.numeric(as.factor(data$ssc_b))-1
data$hsc_b=as.numeric(as.factor(data$hsc_b))-1
data$hsc_s=as.numeric(as.factor(data$hsc_s))-1
data$degree_t=as.numeric(as.factor(data$degree_t))-1
data$workex=as.numeric(as.factor(data$workex))-1
data$status=as.factor(data$status)
data$workex=as.factor(data$workex)
data$specialisation=as.factor(data$specialisation)
data$gender=as.factor(data$gender)
data$ssc_b=as.factor(data$ssc_b)
data$hsc_b=as.factor(data$hsc_b)
data$hsc_s=as.factor(data$hsc_s)
data$degree_t=as.factor(data$degree_t)
str(data)
```
Swapping the last two columns ( predictor variable is moved to the last column )
```{r}
data=data[,c(1,2,3,4,5,6,7,8,9,10,11,12,14,13)]
head(data)
```
Splitting the dataset into training and test data ( In the ratio 67:33 )
```{r}
data_set_size=floor(nrow(data)*0.67)
index<-sample(1:nrow(data),size=data_set_size)
train<-data[index,]
test<-data[-index,]
train
test
```
Building the random forest model :

1.Initially we set the ntree=1 , this implies that a single decision tree is used to predict the status.
Let us check the accuracy of this model and then proceed with the construction of multiple decision trees. 
```{r}
rfd <-randomForest(status~.,data=train, ntree=1,mtry=3,importance=TRUE)
rfd
```
2. The error rate of a single decision tree is displayed above.
Let us now increase the ntree value and find out the accuracy and the error rate of the random forest model.
```{r}
rf <-randomForest(status~.,data=train, ntree= 10,mtry=3,importance=TRUE)
rf
plot(rf)
```
3. We can see clearly that using a random forest model over a single decision tree has decreased the error rate and thereby increased the accuracy.

Evaluating the model by running it on the test dataset :
```{r}
res<-predict(rf,newdata=test,type="class")
confusionMatrix(table(res,test$status))
```
