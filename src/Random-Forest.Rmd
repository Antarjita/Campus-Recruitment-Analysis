---
title: "Exploratory Data Analysis"
output:
  html_document:
    df_print: paged
---
---
Importing the libraries
```{r}
library(randomForest)
library(caTools)
library(caret)
library(dplyr)
```
Reading the dataset
```{r}
set.seed(3654)
data<-read.csv("/Users/saishruthi/documents/sem_5/data_analytics/project/Placement_Data_Full_Class.csv")
head(data)
```
Data Cleaning and Data Pre-processing
1.Dropping the first column 
```{r}
data<-select(data,-1)
head(data)
```

2.Removing Null Values
```{r}
data<-select(data,-14,-6)
head(data)
```
3.Removing Outliers
```{r}
q1<-quantile(data$hsc_p,0.25)
q3<-quantile(data$hsc_p,0.75)
iqr<-1.5*(q3-q1)
data<- data %>% filter(data$hsc_p>=q1-iqr,data$hsc_p<=q3+iqr)
boxplot(data$hsc_p,xlab="after Cleaning 12th Marks")
```

4.Normalization
```{r}
normalize<-function(attribute){ return ( (attribute - min(attribute))/(max(attribute)-min(attribute))) }
data$ssc_p<-normalize(data$ssc_p)
data$hsc_p<-normalize(data$hsc_p)
data$degree_p<-normalize(data$degree_p)
data$etest_p<-normalize(data$etest_p)
data$mba_p<-normalize(data$mba_p)
head(data)
```
Data preview
```{r}
str(data)
```
Handling categorical variables
```{r}
data$status=as.numeric(as.factor(data$status))-1
data$gender=as.numeric(as.factor(data$gender))-1
data$specialisation=as.numeric(as.factor(data$specialisation))-1
data$ssc_b=as.numeric(as.factor(data$ssc_b))-1
data$hsc_b=as.numeric(as.factor(data$hsc_b))-1

data$degree_t=as.numeric(as.factor(data$degree_t))-1
data$workex=as.numeric(as.factor(data$workex))-1
data$status=as.factor(data$status)
data$workex=as.factor(data$workex)
data$specialisation=as.factor(data$specialisation)
data$gender=as.factor(data$gender)
data$ssc_b=as.factor(data$ssc_b)
data$hsc_b=as.factor(data$hsc_b)

data$degree_t=as.factor(data$degree_t)
str(data)
```

Splitting the dataset into training and test data ( In the ratio 67:33 )
```{r}
data_set_size=floor(nrow(data)*0.67)
index<-sample(1:nrow(data),size=data_set_size)
train<-data[index,]
test<-data[-index,]
train
```
```{r}
test
```
Building the random forest model :

1.Initially we set the ntree=1 , this implies that a single decision tree is used to predict the status.
Let us check the accuracy of this model and then proceed with the construction of multiple decision trees. 
```{r}
rfd <-randomForest(status~.,data=train, ntree=1,mtry=3,importance=TRUE)
rfd
```
2. The error rate of a single decision tree is displayed above.
Let us now increase the ntree value and find out the accuracy and the error rate of the random forest model.
```{r}
rf <-randomForest(status~.,data=train, ntree=301,mtry=4,importance=TRUE)
rf
```
```{r}
plot(rf)
```
```{r}
importance(rf)
```
3. We can see clearly that using a random forest model over a single decision tree has decreased the error rate and thereby increased the accuracy.

Evaluating the model by running it on the test dataset :
```{r}
res<-predict(rf,newdata=test,type="class")
confusionMatrix(table(res,test$status))
```
